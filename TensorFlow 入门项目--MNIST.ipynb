{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 机器学习入门\n",
    "---\n",
    "> 引言：编程入门有‘Hello world’ , ML 入门有‘MNIST’\n",
    "\n",
    "### 简介\n",
    "---\n",
    "MNIST 是一个入门级的计算机视觉数据集，他包含：\n",
    "* 各种手写数字图片（x 值 -- 试卷）\n",
    "* 每一张图片对应的标签（y 值 -- 答案）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练/测试数据集获取\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 执行这段 code，回去 执行 input_data.py 这个文件中的方法，download 相关的数据集\n",
    "import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建\n",
    "---\n",
    "$ y = softmax(Wx + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# init x , None means 第一维度可以是 任意长度 的值\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#  模型实现 ( y = Wx + b)\n",
    "#  Wx + b 的结果输入到 tf.nn.softmax() 函数中\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "---\n",
    "这个时候，到<u>**损失函数**</u>登场了\n",
    "\n",
    "一种常见、漂亮的成本函数：\"交叉熵\"（cross-entropy）\n",
    "$$\n",
    "H_y^|(y) =  -\\sum_i{y^|_ilog(y_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# 计算 交叉熵\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了交叉熵（loss(y)）,现在要开始优化啦，用<u>**梯度下降**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 梯度下降算法（gradient descent algorithm）\n",
    "# 学习率 0.01\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型建立完毕，要启动啦\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9214\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # loop 1000 times\n",
    "    for i in range(1000):\n",
    "        # 随机抓取训练数据中的100个批处理数据点\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        # 把真实值给 y_ , input数据给 x ，开始训练\n",
    "        sess.run(train_step, feed_dict={x:batch_xs, y_:batch_ys})\n",
    "    # 评估模型\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    # bool 转 float ----> tf.cast()\n",
    "    # 求平均值 -----> tf.reduct_mean()\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估模型\n",
    "---\n",
    "* `tf.argmax`：一个有用的函数，能给出tensor 对象在某一维度上的其数据最大值所在的索引值。\n",
    "例如：\n",
    "`tf.argmax(y, 1)`表示模型对于任一输入x预测得到的 标签值\n",
    "`tf.argmax(y_, 1)`代表正确的标签\n",
    "用`tf.equal` 检测我们的预测是否真实标签匹配（索引 位置一样表示匹配）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于 Softmax回归模型\n",
    "---\n",
    "特点：\n",
    "* 很火，很广泛使用的一种模型。\n",
    "\n",
    "简单理解：\n",
    "* max：a和b, 且，a>b ,那么直接取a \n",
    "* softmax: 我不想直接取a，我想这时候b也有小概率会取到，说不定b更有用呢？对吧\n",
    "  * 现在还是a和b，a>b， 按照softmax 计算a和b的概率，那a的softmax值大于b的，所以a经常能取到，而b也偶尔会取到，概率和他们本身的大小有关。\n",
    "\n",
    "公式：\n",
    "$$\n",
    "S_i = \\frac{e^{V_i}}{\\sum_{i=1}^n{e^{V_i}}}\n",
    "$$\n",
    "\n",
    "#### 交叉熵（Loss）\n",
    "计算与标注样本的差距时，还是很有用的。\n",
    "下面是交叉熵公式：\n",
    "$$\n",
    "L_i = -log(\\frac{e^{f_{y_i}}}{\\sum_j{e^j}})\n",
    "$$\n",
    "这个$e^{f_{y_i}}$就是指 这一组数据中 正确分类 对应的分子，那么一除，log里面的计算就是指这个正确分类的softmax值，它所占比重越大，整个样本的Loss就越小（这里，你想想log 函数的图形，$log(1)=0$ ，而$log(x<1)$是一个很大的负数）\n",
    "\n",
    "#### 之后就是 ， 梯度下降\n",
    "怎么改进我们的Loss呢？要通过梯度下降，每次走一个step大小的梯度，\n",
    "我们定义：选到$y_i$的概率是：\n",
    "$$\n",
    "P_{y_i} = \\frac{e^{f_{y_i}}}{\\sum_j{e^j}}\n",
    "$$\n",
    "然后我们求Loss对每个权重矩阵的偏导数，用链式法则:\n",
    "$$\n",
    "\\frac{dL_i}{df_{y_i}} = P_{y_i} -1\n",
    "$$\n",
    "最后结果的形式非常的简单，只要将算出来的概率的向量对应的真正结果的那一维减1，就可以了.\n",
    "举个例子：\n",
    "\n",
    "条件：\n",
    "* 若干次训练后，最后得到某个训练样本的向量的分数是[1,5,3]\n",
    "\n",
    "解：\n",
    "* 概率：[0.015, 0.866, 0.117]\n",
    "* 假设样本中正确的分类是第二个：偏导就是：[0.015, 0.866-1, 0.117]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
